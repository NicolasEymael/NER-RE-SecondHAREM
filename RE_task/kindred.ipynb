{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframe from CSV and transform to PubAnnotation/Json format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denotation(start, end, id_counter, tipo):\n",
    "    span = {}\n",
    "    span['begin'] = start\n",
    "    span['end'] = end\n",
    "\n",
    "    denotation = {}\n",
    "    denotation['id'] = \"T{}\".format(id_counter)\n",
    "    denotation['obj'] = tipo\n",
    "    denotation['span'] = span\n",
    "    \n",
    "    return denotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relation(id_counter, tipo_rel, subj_entity, obj_entity):\n",
    "    relation = {}\n",
    "    relation['id'] = \"R{}\".format(id_counter)\n",
    "    relation['pred'] = tipo_rel\n",
    "    relation['subj'] = subj_entity\n",
    "    relation['obj'] = obj_entity\n",
    "    \n",
    "    return relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_denotations_from_sentence(df, sentence, entity_indexes):\n",
    "    all_denotations = []\n",
    "    \n",
    "    denotation_id = 1\n",
    "    \n",
    "    offset = 0\n",
    "    subsentence = sentence\n",
    "    \n",
    "    # dicionario com as entidades que pertencem a essa sentença\n",
    "    # mapeia cada ID de entidade do dataset para o ID correspondente no json do kindred (i.e. T1, T2, T3...)\n",
    "    # vai ser usado depois para verificar se as entidades das relações pertecem ou não a mesma sentença\n",
    "    entity_id_to_kindred_id_dict = {}\n",
    "    \n",
    "    for entity_index in entity_indexes:\n",
    "        entity = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity\"].values[0]\n",
    "        \n",
    "        entity_id = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity_id\"].values[0]\n",
    "        \n",
    "        tipo = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"tipo_final\"].values[0]\n",
    "\n",
    "        # \"limpa\" a sentença das entidades que já foram processadas\n",
    "        entity_start_index = subsentence.index(entity) + offset\n",
    "        entity_end_index = entity_start_index + len(entity)\n",
    "        offset = entity_end_index\n",
    "        subsentence = subsentence[subsentence.index(entity) + len(entity):]\n",
    "\n",
    "        denotation = create_denotation(\n",
    "            entity_start_index,\n",
    "            entity_end_index,\n",
    "            denotation_id,\n",
    "            tipo)\n",
    "        \n",
    "        denotation_id += 1\n",
    "        \n",
    "        all_denotations.append(denotation)\n",
    "        \n",
    "        # adiciona essa entidade no dicionario de entidades da sentença\n",
    "        entity_id_to_kindred_id_dict[entity_id] = denotation['id']\n",
    "        \n",
    "    return all_denotations, entity_id_to_kindred_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denotations_from_doc(df, sentence, doc_index):\n",
    "    all_denotations = []\n",
    "    \n",
    "    denotation_id = 1\n",
    "    \n",
    "    # dicionario que mapeia cada ID de entidade do dataset para o ID correspondente no json do kindred (i.e. T1, T2, T3...)\n",
    "    entity_id_to_kindred_id_dict = {}\n",
    "    \n",
    "    offset = 0\n",
    "    subsentence = sentence\n",
    "    \n",
    "    # lista com todas os IDs das entidades que pertencem a esse documento\n",
    "    entity_ids = df.loc[df.doc_index == doc_index, \"entity_id\"].values\n",
    "    \n",
    "    for entity_id in entity_ids:\n",
    "        entity = df.loc[df.entity_id == entity_id, \"entity\"].values[0]\n",
    "        \n",
    "        tipo = df.loc[df.entity_id == entity_id, \"tipo_final\"].values[0]\n",
    "\n",
    "        # \"limpa\" a sentença das entidades que já foram processadas\n",
    "        entity_start_index = subsentence.index(entity) + offset\n",
    "        entity_end_index = entity_start_index + len(entity)\n",
    "        offset = entity_end_index\n",
    "        subsentence = subsentence[subsentence.index(entity) + len(entity):]\n",
    "\n",
    "        denotation = create_denotation(\n",
    "            entity_start_index,\n",
    "            entity_end_index,\n",
    "            denotation_id,\n",
    "            tipo)\n",
    "        \n",
    "        denotation_id += 1\n",
    "        \n",
    "        all_denotations.append(denotation)\n",
    "        \n",
    "        # adiciona essa entidade no dicionario de entidades da sentença\n",
    "        entity_id_to_kindred_id_dict[entity_id] = denotation['id']\n",
    "        \n",
    "    return all_denotations, entity_id_to_kindred_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relations_from_sentence(df, sentence, entity_indexes, entity_id_to_kindred_id_dict):\n",
    "    all_relations = []\n",
    "    \n",
    "    relation_id = 1\n",
    "    \n",
    "    # se a relação tem uma entidade em uma sentença e outra entidade em outra sentença, essa relação vai ser descartada\n",
    "    # o kindred só é capaz de reconhecer relações entre entidades em uma mesma sentença\n",
    "    discarded_relations = []\n",
    "    \n",
    "    for entity_index in entity_indexes:\n",
    "        entity = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity\"].values[0]\n",
    "        \n",
    "        entity_id = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity_id\"].values[0]\n",
    "        \n",
    "        # corels é uma String com as \"entidades objetos\" de cada relação, separadas por um espaço\n",
    "        # EX: H2-dftre765-12 H2-dftre765-9 H2-dftre765-1\n",
    "        corels = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"corel\"].values[0]\n",
    "        \n",
    "        # para ignorar os casos em que não tem nenhuma relação, fazer esse if x==x:\n",
    "        if (corels == corels):\n",
    "            # tiporels é uma lista com as relações\n",
    "            # EX: [autor_de, natural_de, participante_em]\n",
    "            tiporels = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"tiporel\"].values[0].split()\n",
    "            \n",
    "            # transforma o corels em uma lista e processa elemento a elemento (cada elemento \"corel\" é o id do obj da relação)\n",
    "            for corel_index, corel in enumerate(corels.split()):\n",
    "                \n",
    "                # só processar a relação se o obj está na mesma sentença que o subj, ou seja, está no dict\n",
    "                # a parte do \"corel_index < len(tiporels)\" é porque tem uns dados bugados no dataset\n",
    "                if corel in entity_id_to_kindred_id_dict and corel_index < len(tiporels):\n",
    "                    \n",
    "                    relation = create_relation(\n",
    "                        relation_id,\n",
    "                        tiporels[corel_index],\n",
    "                        entity_id_to_kindred_id_dict[entity_id],\n",
    "                        entity_id_to_kindred_id_dict[corel])\n",
    "                    \n",
    "                    relation_id += 1\n",
    "                    \n",
    "                    all_relations.append(relation)\n",
    "                   \n",
    "                # se não vai utilizar a relação, então adiciona ela na lista de descarte\n",
    "                # a parte do \"len(obj_entity) == 1\" também é por causa de um bug no dataset\n",
    "                else:\n",
    "                    obj_entity = df.loc[df.entity_id == corel, \"entity\"].values\n",
    "                    if (len(obj_entity) == 1):\n",
    "                        text = \"the subj {} ({}) is in sentence {}, but the obj {} ({}) is in other sentence\".format(entity, entity_id, p_index, obj_entity[0], corel)\n",
    "                        discarded_relations.append(text)\n",
    "                        \n",
    "    return all_relations, discarded_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_from_doc(df, sentence, doc_index, entity_id_to_kindred_id_dict):\n",
    "    all_relations = []\n",
    "    \n",
    "    relation_id = 1\n",
    "    \n",
    "    # lista com todas os IDs das entidades que pertencem a esse documento\n",
    "    entity_ids = df.loc[df.doc_index == doc_index, \"entity_id\"].values\n",
    "    \n",
    "    for entity_id in entity_ids:\n",
    "        entity = df.loc[df.entity_id == entity_id, \"entity\"].values[0]\n",
    "        \n",
    "        # corels é uma String com as \"entidades objetos\" de cada relação, separadas por um espaço\n",
    "        # EX: H2-dftre765-12 H2-dftre765-9 H2-dftre765-1\n",
    "        corels = df.loc[df.entity_id == entity_id, \"corel\"].values[0]\n",
    "        \n",
    "        # para ignorar os casos em que não tem nenhuma relação, fazer esse if x==x:\n",
    "        if (corels == corels):\n",
    "            # tiporels é uma lista com as relações\n",
    "            # EX: [autor_de, natural_de, participante_em]\n",
    "            tiporels = df.loc[df.entity_id == entity_id, \"tiporel\"].values[0].split()\n",
    "            \n",
    "            # transforma o corels em uma lista e processa elemento a elemento (cada elemento \"corel\" é o id do obj da relação)\n",
    "            for corel_index, corel in enumerate(corels.split()):\n",
    "                \n",
    "                # só processar a relação se o obj está no dict\n",
    "                # a parte do \"corel_index < len(tiporels)\" é porque tem uns dados bugados no dataset\n",
    "                if corel in entity_id_to_kindred_id_dict and corel_index < len(tiporels):\n",
    "                    \n",
    "                    relation = create_relation(\n",
    "                        relation_id,\n",
    "                        tiporels[corel_index],\n",
    "                        entity_id_to_kindred_id_dict[entity_id],\n",
    "                        entity_id_to_kindred_id_dict[corel])\n",
    "                    \n",
    "                    relation_id += 1\n",
    "                    \n",
    "                    all_relations.append(relation)\n",
    "                        \n",
    "    return all_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach: create a Json file for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"dataset_processed_6.csv\")\n",
    "\n",
    "all_discarded_relations = []\n",
    "\n",
    "# p_index ranges from 1 to 2273 (total number of sentences)\n",
    "p_indexes_set = set(df[\"p_index\"])\n",
    "\n",
    "for p_index in p_indexes_set:\n",
    "    \n",
    "    sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "    \n",
    "    # lista com a quantidade de entidades que tem na sentença\n",
    "    entity_indexes = df.loc[df.p_index == p_index, \"entity_index\"].values\n",
    "    \n",
    "    all_denotations, entity_id_to_kindred_id_dict = get_all_denotations_from_sentence(df,\n",
    "                                                                                      sentence,\n",
    "                                                                                      entity_indexes)\n",
    "    \n",
    "    all_relations, discarded_relations = get_all_relations_from_sentence(df, \n",
    "                                                                         sentence,\n",
    "                                                                         entity_indexes,\n",
    "                                                                         entity_id_to_kindred_id_dict)\n",
    "    \n",
    "    all_discarded_relations.append(discarded_relations)\n",
    "        \n",
    "    # cada arquivo json vai ser \"sentença + entidades + relações\"\n",
    "    json_per_sentence = {}     \n",
    "    json_per_sentence['text'] = sentence\n",
    "    json_per_sentence['denotations'] = all_denotations\n",
    "    json_per_sentence['relations'] = all_relations\n",
    "    \n",
    "    with open(\"kindred/sentence_p_index_{}.json\".format(p_index), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "        json.dump(json_per_sentence, fileWriter, indent=2, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "# salva em um arquivo todas as relações que foram descartadas\n",
    "with open(\"kindred/discarded_relations.txt\", \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "    for discarded_relations in all_discarded_relations:\n",
    "        for text in discarded_relations:\n",
    "            print(text, file=fileWriter)\n",
    "            \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach: create a Json file for each document (so we can use all relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"dataset_processed_6.csv\")\n",
    "\n",
    "all_discarded_relations = []\n",
    "\n",
    "# doc_index ranges from 0 to 128 (total number of documents)\n",
    "doc_indexes_set = set(df[\"doc_index\"])\n",
    "\n",
    "for doc_index in sorted(doc_indexes_set):\n",
    "    \n",
    "    big_sentence = \"\"\n",
    "    \n",
    "    # total number of sentences in document\n",
    "    p_indexes_set = set(df.loc[df.doc_index == doc_index, \"p_index\"].values)\n",
    "    \n",
    "    # build a big sentence with all sentences from document\n",
    "    for p_index in sorted(p_indexes_set):\n",
    "        sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "\n",
    "        if (sentence.endswith('.')):\n",
    "            big_sentence = big_sentence + \" \" + sentence\n",
    "        else:\n",
    "            big_sentence = big_sentence + \" \" + sentence + \".\"\n",
    "    \n",
    "    print(sorted(p_indexes_set))\n",
    "    #print(big_sentence)\n",
    "    print(\"\\n\")\n",
    "    all_denotations, ids_dict = get_denotations_from_doc(df, big_sentence, doc_index)\n",
    "\n",
    "    all_relations = get_relations_from_doc(df, big_sentence, doc_index, ids_dict)\n",
    "\n",
    "    big_json = {}\n",
    "    big_json['text'] = big_sentence\n",
    "    big_json['denotations'] = all_denotations\n",
    "    big_json['relations'] = all_relations\n",
    "\n",
    "\n",
    "    with open(\"kindred/big_sentence_doc_{}.json\".format(doc_index), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "        json.dump(big_json, fileWriter, indent=2, ensure_ascii=False)\n",
    "        \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Approach: create a Json file for each relation (with big sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "relation_counter = 0\n",
    "\n",
    "df = pd.read_csv(\"dataset_processed_6.csv\")\n",
    "\n",
    "# doc_index ranges from 0 to 128 (total number of documents)\n",
    "doc_indexes_set = set(df[\"doc_index\"])\n",
    "\n",
    "for doc_index in sorted(doc_indexes_set):\n",
    "    \n",
    "    big_sentence = \"\"\n",
    "    \n",
    "    # total number of sentences in document\n",
    "    p_indexes_set = set(df.loc[df.doc_index == doc_index, \"p_index\"].values)\n",
    "    \n",
    "    # build a big sentence with all sentences from document\n",
    "    for p_index in sorted(p_indexes_set):\n",
    "        sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "\n",
    "        if (sentence.endswith('.')):\n",
    "            big_sentence = big_sentence + \" \" + sentence\n",
    "        else:\n",
    "            big_sentence = big_sentence + \" \" + sentence + \".\"\n",
    "    \n",
    "    all_denotations, entity_id_to_kindred_id_dict = get_denotations_from_doc(df, big_sentence, doc_index)\n",
    "\n",
    "    all_relations = get_relations_from_doc(df, big_sentence, doc_index, entity_id_to_kindred_id_dict)\n",
    "\n",
    "    # aqui nós já temos todas as entidades e relações do documento...\n",
    "    # pegar cada uma das relações e criar um json que só tenha 1 relação e 2 entidades\n",
    "    for relation in all_relations:\n",
    "        # pega o id TX das entidades sujeito e objeto\n",
    "        subj_kindred_id = relation['subj']\n",
    "        obj_kindred_id = relation['obj']\n",
    "\n",
    "        # pega as denotations entidades sujeito e objeto\n",
    "        for denotation in all_denotations:\n",
    "            if (denotation['id'] == subj_kindred_id):\n",
    "                subj_denotation = denotation.copy()\n",
    "            elif (denotation['id'] == obj_kindred_id):\n",
    "                obj_denotation = denotation.copy()\n",
    "\n",
    "        # substitui os IDs para começarem do 1\n",
    "        relation['id'] = \"R1\"\n",
    "        relation['subj'] = \"T1\"\n",
    "        relation['obj'] = \"T2\"\n",
    "        subj_denotation['id'] = \"T1\"\n",
    "        obj_denotation['id'] = \"T2\"\n",
    "\n",
    "        big_json = {}\n",
    "        big_json['text'] = big_sentence\n",
    "        big_json['denotations'] = [subj_denotation, obj_denotation]\n",
    "        big_json['relations'] = [relation]\n",
    "        \n",
    "        with open(\"teste/big_sentence_doc_{}.json\".format(relation_counter), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "            json.dump(big_json, fileWriter, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        relation_counter += 1\n",
    "                                   \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA STEP:\n",
    "Salvar os arquivos no Drive e executar o Kindred_Experiments pelo Google Colab.\n",
    "\n",
    "Depois de pegar os arquivos que foram gerados pelo modelo, pode seguir pro próximo passo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELATION EXTRACTION WITH KINDRED\n",
    "A execução do relation_by_relation acabou estourando a memória do Google Colab, então tive que executar localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kindred\n",
    "!python -m spacy download pt_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kindred\n",
    "import time\n",
    "import sys\n",
    "\n",
    "default_stdout = sys.stdout\n",
    "\n",
    "fold_count = 10\n",
    "\n",
    "# o load estava causando alguns problemas de codificação pelo Jupyter (pelo Colab não dava problema)\n",
    "# eu abri o código interno da lib kindred e alterei uma parte do código que era \"open(filename)\" por \"open(filename, utf-8)\"\n",
    "mainCorpus = kindred.load(\"pubannotation\", \"kindred_relation_by_relation\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "if True:\n",
    "    for trainCorpus, goldCorpus in mainCorpus.nfold_split(fold_count):\n",
    "\n",
    "        sys.stdout = open(\"results_big_fold_{}.txt\".format(i), \"w+\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        classifier = kindred.RelationClassifier(model=\"pt_core_news_md\")\n",
    "\n",
    "        classifier.train(trainCorpus)\n",
    "\n",
    "        testCorpus = goldCorpus.clone()\n",
    "\n",
    "        testCorpus.removeRelations()\n",
    "\n",
    "        classifier.predict(testCorpus)\n",
    "\n",
    "        kindred.evaluate(goldCorpus, testCorpus, metric='all', display=True)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"--- {} seconds ---\".format(end_time - start_time))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        sys.stdout.close()\n",
    "\n",
    "    sys.stdout = default_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Stratified Folds \n",
    "Isso não é mais utilizado.\n",
    "Esse código foi usado para separar os folds quando o dataset inteiro foi centralizado em uma única sentença. No entanto, o Kindred acertou 0 TPs nesse caso, então optei por fazer de outra maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    import json\n",
    "\n",
    "    ids = [relation[\"id\"] for relation in all_relations] # na verdade isso nem precisa, na doc disse que só as labels já servem\n",
    "\n",
    "    tiporels = [relation[\"pred\"] for relation in all_relations]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for train_indexes, test_indexes in skf.split(ids, tiporels):\n",
    "\n",
    "        train_fold = [all_relations[index] for index in train_indexes]\n",
    "\n",
    "        test_fold = [all_relations[index] for index in test_indexes]\n",
    "\n",
    "        train_json = {}\n",
    "        train_json['text'] = big_sentence\n",
    "        train_json['denotations'] = all_denotations\n",
    "        train_json['relations'] = train_fold\n",
    "\n",
    "        test_json = {}\n",
    "        test_json['text'] = big_sentence\n",
    "        test_json['denotations'] = all_denotations\n",
    "        test_json['relations'] = test_fold\n",
    "\n",
    "        with open(\"kindred_folds/train_fold_{}.json\".format(i), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "            json.dump(train_json, fileWriter, indent=2, ensure_ascii=False)\n",
    "\n",
    "        with open(\"kindred_folds/test_fold_{}.json\".format(i), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "            json.dump(test_json, fileWriter, indent=2, ensure_ascii=False)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULAR METRICAS MICRO E MACRO A PARTIR DOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "P = 0.6811679\n",
      "R = 0.6843985\n",
      "F = 0.669432\n",
      "\n",
      "\n",
      "1\n",
      "P = 0.665911\n",
      "R = 0.6149571999999999\n",
      "F = 0.6169362\n",
      "\n",
      "\n",
      "2\n",
      "P = 0.6664977999999999\n",
      "R = 0.6337047\n",
      "F = 0.6368026\n",
      "\n",
      "\n",
      "3\n",
      "P = 0.6876503999999999\n",
      "R = 0.5913721\n",
      "F = 0.6044445\n",
      "\n",
      "\n",
      "4\n",
      "P = 0.6533356000000001\n",
      "R = 0.6265207999999999\n",
      "F = 0.6222332999999999\n",
      "\n",
      "\n",
      "5\n",
      "P = 0.6170353000000001\n",
      "R = 0.5927911\n",
      "F = 0.5915109000000001\n",
      "\n",
      "\n",
      "6\n",
      "P = 0.7122315000000001\n",
      "R = 0.6621200999999999\n",
      "F = 0.6643102999999999\n",
      "\n",
      "\n",
      "7\n",
      "P = 0.7066344\n",
      "R = 0.6118888\n",
      "F = 0.6237676000000001\n",
      "\n",
      "\n",
      "8\n",
      "P = 0.7071309\n",
      "R = 0.6307379\n",
      "F = 0.6418549\n",
      "\n",
      "\n",
      "9\n",
      "P = 0.6399335\n",
      "R = 0.5899413000000001\n",
      "F = 0.6015599\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "\n",
    "    with open(\"dbpedia/metrics{}.txt\".format(fold), \"r\", encoding=\"utf-8\") as fileReader:\n",
    "        i = 0\n",
    "        dicP = []\n",
    "        dicR = []\n",
    "        dicF = []\n",
    "\n",
    "        for line in fileReader:\n",
    "            if(line.partition(\"----\")[1]):\n",
    "                break\n",
    "            else:\n",
    "                texto1 = line.partition(\"\\tP:\")[2]\n",
    "                precision = texto1.partition(\" R:\")[0]\n",
    "                texto2 = texto1.partition(\" R:\")[2]\n",
    "                recall = texto2.partition(\" F1:\")[0]\n",
    "                fscore = texto2.partition(\" F1:\")[2]\n",
    "\n",
    "                if (False):\n",
    "                    print(line)\n",
    "                    print(\"P = {}\".format(precision))\n",
    "                    print(\"R = {}\".format(recall))\n",
    "                    print(\"F = {}\".format(fscore))\n",
    "\n",
    "                dicP.append(float(precision))\n",
    "                dicR.append(float(recall))\n",
    "                dicF.append(float(fscore))\n",
    "\n",
    "                i += 1\n",
    "        \n",
    "        print(fold)\n",
    "        textP = \"P = {}\".format(sum(dicP)/len(dicP))\n",
    "        print(textP)\n",
    "        textR = \"R = {}\".format(sum(dicR)/len(dicR))\n",
    "        print(textR)\n",
    "        textF = \"F = {}\".format(sum(dicF)/len(dicF))\n",
    "        print(textF)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
