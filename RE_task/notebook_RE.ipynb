{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURATION\n",
    "#\n",
    "# which Run? 1, 2 or 3\n",
    "#chosen_run = 1 # one JSON per sentence\n",
    "chosen_run = 2 # one JSON per document\n",
    "#chosen_run = 3 # one JSON per relation\n",
    "#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denotation(start, end, id_counter, tipo):\n",
    "    span = {}\n",
    "    span['begin'] = start\n",
    "    span['end'] = end\n",
    "\n",
    "    denotation = {}\n",
    "    denotation['id'] = \"T{}\".format(id_counter)\n",
    "    denotation['obj'] = tipo\n",
    "    denotation['span'] = span\n",
    "    \n",
    "    return denotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relation(id_counter, tipo_rel, subj_entity, obj_entity):\n",
    "    relation = {}\n",
    "    relation['id'] = \"R{}\".format(id_counter)\n",
    "    relation['pred'] = tipo_rel\n",
    "    relation['subj'] = subj_entity\n",
    "    relation['obj'] = obj_entity\n",
    "    \n",
    "    return relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denotations_from_sentence(df, sentence, entity_indexes):\n",
    "    all_denotations = []\n",
    "    \n",
    "    denotation_id = 1\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    subsentence = sentence\n",
    "    \n",
    "    # dictionary with the entities that belong to this sentence\n",
    "    # maps each entity ID from the dataset to the corresponding Kindred JSON ID (i.e. T1, T2, T3 ...)\n",
    "    # this dictionary will be used later to check whether the entities in each relation belong to the same sentence or not\n",
    "    entity_id_to_kindred_id_dict = {}\n",
    "    \n",
    "    for entity_index in entity_indexes:\n",
    "        entity = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity\"].values[0]\n",
    "        \n",
    "        entity_id = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity_id\"].values[0]\n",
    "        \n",
    "        tipo = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"tipo_final\"].values[0]\n",
    "\n",
    "        # \"clears\" the sentence of entities that have already been processed\n",
    "        entity_start_index = subsentence.index(entity) + offset\n",
    "        entity_end_index = entity_start_index + len(entity)\n",
    "        offset = entity_end_index\n",
    "        subsentence = subsentence[subsentence.index(entity) + len(entity):]\n",
    "\n",
    "        denotation = create_denotation(\n",
    "            entity_start_index,\n",
    "            entity_end_index,\n",
    "            denotation_id,\n",
    "            tipo)\n",
    "        \n",
    "        denotation_id += 1\n",
    "        \n",
    "        all_denotations.append(denotation)\n",
    "        \n",
    "        # add this entity to the dictionary of the sentence\n",
    "        entity_id_to_kindred_id_dict[entity_id] = denotation['id']\n",
    "        \n",
    "    return all_denotations, entity_id_to_kindred_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denotations_from_doc(df, sentence, doc_index):\n",
    "    all_denotations = []\n",
    "    \n",
    "    denotation_id = 1\n",
    "    \n",
    "    # dictionary that maps each entity ID from the dataset to the corresponding Kindred JSON ID (i.e. T1, T2, T3 ...)\n",
    "    entity_id_to_kindred_id_dict = {}\n",
    "    \n",
    "    offset = 0\n",
    "    subsentence = sentence\n",
    "    \n",
    "    # list with all the IDs of the entities that belong to this document\n",
    "    entity_ids = df.loc[df.doc_index == doc_index, \"entity_id\"].values\n",
    "    \n",
    "    for entity_id in entity_ids:\n",
    "        entity = df.loc[df.entity_id == entity_id, \"entity\"].values[0]\n",
    "        \n",
    "        tipo = df.loc[df.entity_id == entity_id, \"tipo_final\"].values[0]\n",
    "\n",
    "        # \"clears\" the sentence of entities that have already been processed\n",
    "        entity_start_index = subsentence.index(entity) + offset\n",
    "        entity_end_index = entity_start_index + len(entity)\n",
    "        offset = entity_end_index\n",
    "        subsentence = subsentence[subsentence.index(entity) + len(entity):]\n",
    "\n",
    "        denotation = create_denotation(\n",
    "            entity_start_index,\n",
    "            entity_end_index,\n",
    "            denotation_id,\n",
    "            tipo)\n",
    "        \n",
    "        denotation_id += 1\n",
    "        \n",
    "        all_denotations.append(denotation)\n",
    "        \n",
    "        # add this entity to the dictionary of the sentence\n",
    "        entity_id_to_kindred_id_dict[entity_id] = denotation['id']\n",
    "        \n",
    "    return all_denotations, entity_id_to_kindred_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_from_sentence(df, sentence, entity_indexes, entity_id_to_kindred_id_dict):\n",
    "    all_relations = []\n",
    "    \n",
    "    relation_id = 1\n",
    "    \n",
    "    # if the relation has one entity in one sentence and another entity in another sentence, that relation will be discarded\n",
    "    # kindred is only able to recognize relations between entities in the same sentence\n",
    "    discarded_relations = []\n",
    "    \n",
    "    for entity_index in entity_indexes:\n",
    "        entity = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity\"].values[0]\n",
    "        \n",
    "        entity_id = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"entity_id\"].values[0]\n",
    "        \n",
    "        # corels is a String with the \"object entities\" of each relation, separated by a space\n",
    "        # EX: H2-dftre765-12 H2-dftre765-9 H2-dftre765-1\n",
    "        corels = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"corel\"].values[0]\n",
    "        \n",
    "        # ignore cases where there is no relation\n",
    "        if (corels == corels):\n",
    "            # tiporels is a list of relations\n",
    "            # EX: [autor_de, natural_de, participante_em]\n",
    "            tiporels = df.loc[(df.p_index == p_index) & (df.entity_index == entity_index), \"tiporel\"].values[0].split()\n",
    "            \n",
    "            # make corels a list and process element by element (each element \"corel\" is the object id of the relation)\n",
    "            for corel_index, corel in enumerate(corels.split()):\n",
    "                # process the relation only if the obj is in the same sentence as the subj AKA it is in the dict\n",
    "                # the verification \"corel_index<len (tiporels)\" is because there is some buggy data in the dataset   \n",
    "                if corel in entity_id_to_kindred_id_dict and corel_index < len(tiporels):\n",
    "                    \n",
    "                    relation = create_relation(\n",
    "                        relation_id,\n",
    "                        tiporels[corel_index],\n",
    "                        entity_id_to_kindred_id_dict[entity_id],\n",
    "                        entity_id_to_kindred_id_dict[corel])\n",
    "                    \n",
    "                    relation_id += 1\n",
    "                    \n",
    "                    all_relations.append(relation)\n",
    "                   \n",
    "                # add the unused relations to the discard list\n",
    "                # the verification \"len (obj_entity) == 1\" is also due to buggy data in the dataset   \n",
    "                else:\n",
    "                    obj_entity = df.loc[df.entity_id == corel, \"entity\"].values\n",
    "                    if (len(obj_entity) == 1):\n",
    "                        text = \"the subj {} ({}) is in sentence {}, but the obj {} ({}) is in other sentence\".format(entity, entity_id, p_index, obj_entity[0], corel)\n",
    "                        discarded_relations.append(text)\n",
    "                        \n",
    "    return all_relations, discarded_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_from_doc(df, sentence, doc_index, entity_id_to_kindred_id_dict):\n",
    "    all_relations = []\n",
    "    \n",
    "    relation_id = 1\n",
    "    \n",
    "    # list with all the IDs of the entities that belong to this document\n",
    "    entity_ids = df.loc[df.doc_index == doc_index, \"entity_id\"].values\n",
    "    \n",
    "    for entity_id in entity_ids:\n",
    "        entity = df.loc[df.entity_id == entity_id, \"entity\"].values[0]\n",
    "        \n",
    "        # corels is a String with the \"object entities\" of each relation, separated by a space\n",
    "        # EX: H2-dftre765-12 H2-dftre765-9 H2-dftre765-1\n",
    "        corels = df.loc[df.entity_id == entity_id, \"corel\"].values[0]\n",
    "        \n",
    "        # ignore cases where there is no relation\n",
    "        if (corels == corels):\n",
    "            # tiporels is a list of relations\n",
    "            # EX: [autor_de, natural_de, participante_em]\n",
    "            tiporels = df.loc[df.entity_id == entity_id, \"tiporel\"].values[0].split()\n",
    "            \n",
    "            # make corels a list and process element by element (each element \"corel\" is the object id of the relation)\n",
    "            for corel_index, corel in enumerate(corels.split()):\n",
    "                # process the relation only if the obj is in the dict\n",
    "                # the verification \"corel_index<len (tiporels)\" is because there is some buggy data in the dataset \n",
    "                if corel in entity_id_to_kindred_id_dict and corel_index < len(tiporels):\n",
    "                    \n",
    "                    relation = create_relation(\n",
    "                        relation_id,\n",
    "                        tiporels[corel_index],\n",
    "                        entity_id_to_kindred_id_dict[entity_id],\n",
    "                        entity_id_to_kindred_id_dict[corel])\n",
    "                    \n",
    "                    relation_id += 1\n",
    "                    \n",
    "                    all_relations.append(relation)\n",
    "                        \n",
    "    return all_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframe from CSV and transform to PubAnnotation/Json format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 1: create a Json file for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (chosen_run == 1):\n",
    "    df = pd.read_csv(\"../Dataset/dataset_processed.csv\")\n",
    "\n",
    "    all_discarded_relations = []\n",
    "\n",
    "    # p_index ranges from 1 to 2273 (total number of sentences)\n",
    "    p_indexes_set = set(df[\"p_index\"])\n",
    "\n",
    "    for p_index in p_indexes_set:\n",
    "\n",
    "        sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "\n",
    "        # lista com a quantidade de entidades que tem na sentença\n",
    "        entity_indexes = df.loc[df.p_index == p_index, \"entity_index\"].values\n",
    "\n",
    "        all_denotations, entity_id_to_kindred_id_dict = get_denotations_from_sentence(df,\n",
    "                                                                                          sentence,\n",
    "                                                                                          entity_indexes)\n",
    "\n",
    "        all_relations, discarded_relations = get_relations_from_sentence(df, \n",
    "                                                                             sentence,\n",
    "                                                                             entity_indexes,\n",
    "                                                                             entity_id_to_kindred_id_dict)\n",
    "\n",
    "        all_discarded_relations.append(discarded_relations)\n",
    "\n",
    "        # each json file will have \"sentence + entities + relations\"\n",
    "        json_per_sentence = {}     \n",
    "        json_per_sentence['text'] = sentence\n",
    "        json_per_sentence['denotations'] = all_denotations\n",
    "        json_per_sentence['relations'] = all_relations\n",
    "\n",
    "        with open(\"run1/sentence_p_index_{}.json\".format(p_index), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "            json.dump(json_per_sentence, fileWriter, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    # saves all relations that have been discarded to a file\n",
    "    with open(\"run1/discarded_relations.txt\", \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "        for discarded_relations in all_discarded_relations:\n",
    "            for text in discarded_relations:\n",
    "                print(text, file=fileWriter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 2: create a Json file for each document (so we can use all relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (chosen_run == 2):\n",
    "    df = pd.read_csv(\"../Dataset/dataset_processed.csv\")\n",
    "\n",
    "    all_discarded_relations = []\n",
    "\n",
    "    # doc_index ranges from 0 to 128 (total number of documents)\n",
    "    doc_indexes_set = set(df[\"doc_index\"])\n",
    "\n",
    "    for doc_index in sorted(doc_indexes_set):\n",
    "\n",
    "        big_sentence = \"\"\n",
    "\n",
    "        # total number of sentences in document\n",
    "        p_indexes_set = set(df.loc[df.doc_index == doc_index, \"p_index\"].values)\n",
    "\n",
    "        # build a big sentence with all sentences from document\n",
    "        for p_index in sorted(p_indexes_set):\n",
    "            sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "\n",
    "            if (sentence.endswith('.')):\n",
    "                big_sentence = big_sentence + \" \" + sentence\n",
    "            else:\n",
    "                big_sentence = big_sentence + \" \" + sentence + \".\"\n",
    "\n",
    "        all_denotations, ids_dict = get_denotations_from_doc(df, big_sentence, doc_index)\n",
    "\n",
    "        all_relations = get_relations_from_doc(df, big_sentence, doc_index, ids_dict)\n",
    "\n",
    "        big_json = {}\n",
    "        big_json['text'] = big_sentence\n",
    "        big_json['denotations'] = all_denotations\n",
    "        big_json['relations'] = all_relations\n",
    "\n",
    "        with open(\"run2/doc_index_{}.json\".format(doc_index), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "            json.dump(big_json, fileWriter, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 3: create a Json file for each relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (chosen_run == 3):\n",
    "    df = pd.read_csv(\"../Dataset/dataset_processed.csv\")\n",
    "\n",
    "    relation_counter = 0\n",
    "\n",
    "    # doc_index ranges from 0 to 128 (total number of documents)\n",
    "    doc_indexes_set = set(df[\"doc_index\"])\n",
    "\n",
    "    for doc_index in sorted(doc_indexes_set):\n",
    "\n",
    "        big_sentence = \"\"\n",
    "\n",
    "        # total number of sentences in document\n",
    "        p_indexes_set = set(df.loc[df.doc_index == doc_index, \"p_index\"].values)\n",
    "\n",
    "        # build a big sentence with all sentences from document\n",
    "        for p_index in sorted(p_indexes_set):\n",
    "            sentence = df.loc[df.p_index == p_index, \"p_sentence_processed\"].values[0]\n",
    "\n",
    "            if (sentence.endswith('.')):\n",
    "                big_sentence = big_sentence + \" \" + sentence\n",
    "            else:\n",
    "                big_sentence = big_sentence + \" \" + sentence + \".\"\n",
    "\n",
    "        all_denotations, entity_id_to_kindred_id_dict = get_denotations_from_doc(df, big_sentence, doc_index)\n",
    "\n",
    "        all_relations = get_relations_from_doc(df, big_sentence, doc_index, entity_id_to_kindred_id_dict)\n",
    "\n",
    "        # here we already have all the entities and relations of the document ...\n",
    "        # take each of the relations and create a JSON file that only has 1 relation and 2 entities\n",
    "        for relation in all_relations:\n",
    "            # get the id (Tx) of the subject and object entities\n",
    "            subj_kindred_id = relation['subj']\n",
    "            obj_kindred_id = relation['obj']\n",
    "\n",
    "            # take the denotations (subject and object entities)\n",
    "            for denotation in all_denotations:\n",
    "                if (denotation['id'] == subj_kindred_id):\n",
    "                    subj_denotation = denotation.copy()\n",
    "                elif (denotation['id'] == obj_kindred_id):\n",
    "                    obj_denotation = denotation.copy()\n",
    "\n",
    "            # replace IDs to start from 1\n",
    "            relation['id'] = \"R1\"\n",
    "            relation['subj'] = \"T1\"\n",
    "            relation['obj'] = \"T2\"\n",
    "            subj_denotation['id'] = \"T1\"\n",
    "            obj_denotation['id'] = \"T2\"\n",
    "\n",
    "            big_json = {}\n",
    "            big_json['text'] = big_sentence\n",
    "            big_json['denotations'] = [subj_denotation, obj_denotation]\n",
    "            big_json['relations'] = [relation]\n",
    "\n",
    "            with open(\"run3/relation_index_{}.json\".format(relation_counter), \"w+\", encoding=\"utf-8\") as fileWriter:\n",
    "                json.dump(big_json, fileWriter, indent=2, ensure_ascii=False)\n",
    "\n",
    "            relation_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA STEP:\n",
    "The next step will be done in the Google Colab environment.\n",
    "\n",
    "Save JSON files to Google Drive and run colab_Kindred from Colab.\n",
    "\n",
    "Take the files with the results and save them in the folder \"runX_results\".\n",
    "\n",
    "After saving all the files, you can proceed to the next step.\n",
    "\n",
    "OBS: Run 3 ended up exceeding the memory in Google Colab, so I had to run it locally (notebook_Kindred)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATE MACRO METRICS FROM RESULTS\n",
    "MICRO metrics are already in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "P = 0.4227953703703704\n",
      "R = 0.18460962962962965\n",
      "F = 0.23466196296296293\n",
      "\n",
      "\n",
      "1\n",
      "P = 0.3376473461538462\n",
      "R = 0.1556363846153846\n",
      "F = 0.19240634615384614\n",
      "\n",
      "\n",
      "2\n",
      "P = 0.408187125\n",
      "R = 0.17130583333333335\n",
      "F = 0.22988916666666662\n",
      "\n",
      "\n",
      "3\n",
      "P = 0.33422672\n",
      "R = 0.15622392\n",
      "F = 0.19618123999999998\n",
      "\n",
      "\n",
      "4\n",
      "P = 0.25100730769230767\n",
      "R = 0.10360723076923076\n",
      "F = 0.13735165384615383\n",
      "\n",
      "\n",
      "5\n",
      "P = 0.3489621428571428\n",
      "R = 0.14185178571428572\n",
      "F = 0.18467007142857142\n",
      "\n",
      "\n",
      "6\n",
      "P = 0.347198\n",
      "R = 0.17850613793103448\n",
      "F = 0.21825382758620687\n",
      "\n",
      "\n",
      "7\n",
      "P = 0.35917028\n",
      "R = 0.14306976000000002\n",
      "F = 0.19165272000000003\n",
      "\n",
      "\n",
      "8\n",
      "P = 0.437477962962963\n",
      "R = 0.16179711111111114\n",
      "F = 0.22170718518518523\n",
      "\n",
      "\n",
      "9\n",
      "P = 0.3884756538461538\n",
      "R = 0.15152119230769232\n",
      "F = 0.20156484615384615\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (chosen_run == 1):\n",
    "    src_folder = \"run1_results/\"\n",
    "    n_folds = 10\n",
    "elif (chosen_run == 2):\n",
    "    src_folder = \"run2_results/\"\n",
    "    n_folds = 8\n",
    "elif (chosen_run == 3):\n",
    "    src_folder = \"run3_results/\"\n",
    "    n_folds = 10\n",
    "\n",
    "for fold in range(n_folds):\n",
    "\n",
    "    with open(src_folder + \"results_fold_{}.txt\".format(fold), \"r\", encoding=\"utf-8\") as fileReader:\n",
    "        i = 0\n",
    "        dicP = []\n",
    "        dicR = []\n",
    "        dicF = []\n",
    "\n",
    "        for line in fileReader:\n",
    "            if(line.partition(\"----\")[1]):\n",
    "                break\n",
    "            else:\n",
    "                texto1 = line.partition(\"\\tP:\")[2]\n",
    "                precision = texto1.partition(\" R:\")[0]\n",
    "                texto2 = texto1.partition(\" R:\")[2]\n",
    "                recall = texto2.partition(\" F1:\")[0]\n",
    "                fscore = texto2.partition(\" F1:\")[2]\n",
    "\n",
    "                if (False):\n",
    "                    print(line)\n",
    "                    print(\"P = {}\".format(precision))\n",
    "                    print(\"R = {}\".format(recall))\n",
    "                    print(\"F = {}\".format(fscore))\n",
    "\n",
    "                dicP.append(float(precision))\n",
    "                dicR.append(float(recall))\n",
    "                dicF.append(float(fscore))\n",
    "\n",
    "                i += 1\n",
    "        \n",
    "        print(fold)\n",
    "        textP = \"P = {}\".format(sum(dicP)/len(dicP))\n",
    "        print(textP)\n",
    "        textR = \"R = {}\".format(sum(dicR)/len(dicR))\n",
    "        print(textR)\n",
    "        textF = \"F = {}\".format(sum(dicF)/len(dicF))\n",
    "        print(textF)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
